# 문자 단위 언어 모델링

이 프로젝트는 RNN과 LSTM을 사용하여 문자 단위 언어 모델을 구축하는 것입니다. 모델들은 셰익스피어 데이터셋을 훈련하여 텍스트를 생성합니다.

## 개요

- `dataset.py`: 텍스트 데이터를 전처리하고 데이터 로더를 생성하는 `Shakespeare` 데이터셋 클래스를 포함합니다.
- `model.py`: `CharRNN` 및 `CharLSTM` 모델의 구현을 포함합니다.
- `train.py`: 모델을 훈련시키고 훈련 및 검증 손실을 플로팅합니다.
- `generate.py`: 시드 문자와 온도 설정을 기반으로 텍스트를 생성하는 훈련된 모델을 사용합니다.

# 파일 설명

### `dataset.py`

이 파일에는 텍스트 데이터를 전처리하고 데이터 로더를 생성하는 `Shakespeare` 데이터셋 클래스가 포함되어 있습니다. 데이터는 문자 인덱스로 변환되고, 학습을 위해 길이 30의 시퀀스로 분할됩니다.

### `model.py`

이 파일에는 문자 단위 언어 모델의 신경망 구조를 정의하는 `CharRNN` 및 `CharLSTM` 모델 클래스가 포함되어 있습니다.

### `train.py`

이 파일에는 모델을 훈련하고 검증하는 함수가 포함되어 있습니다. 셰익스피어 데이터셋으로 모델을 훈련시키고, 훈련 및 검증 손실을 플로팅합니다.

### `generate.py`

이 파일에는 텍스트 생성 함수가 포함되어 있습니다. 주어진 시드 문자와 온도 설정을 기반으로 훈련된 모델을 사용하여 텍스트를 생성합니다.

## 요구 사항

- Python 3.x
- PyTorch
- NumPy
- Matplotlib


## 훈련 및 검증 손실
다음 그래프는 CharRNN 및 CharLSTM 모델의 훈련 손실을 나타냅니다:

![image](https://github.com/undraa0309/Language-model/assets/133347765/20c75bfd-8ff0-490e-bf36-46d19f20cf9e)



## 텍스트 생성
다음은 모델에 의해 생성된 텍스트 샘플입니다:

### Vanilla RNN
- 시드: 'H', 온도: 1.0

### LSTM
- 시드: 'T', 온도: 1.5

## Softmax 온도
온도 매개변수 `T`를 사용한 소프트맥스 함수는 다음과 같습니다:

\[ y_i = \frac{\exp(z_i / T)}{\sum{\exp(z_i / T)}} \]

다양한 온도 설정은 생성된 텍스트의 무작위성에 영향을 미칩니다. 높은 온도는 더 무작위적인 텍스트를 생성하고, 낮은 온도는 더 예측 가능한 텍스트를 생성합니다.

