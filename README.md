# 문자 단위 언어 모델링

## 개요
이 프로젝트는 순환 신경망(RNN)과 LSTM을 사용하여 문자 단위 언어 모델을 구축하는 것입니다. 이 모델들은 셰익스피어 데이터셋을 훈련하여 텍스트를 생성합니다.

## 파일 설명
- `dataset.py`: 텍스트 데이터를 전처리하고 데이터 로더를 생성하는 `TextDataset` 클래스를 포함합니다.
- `model.py`: `VanillaRNN` 및 `LSTM` 모델의 구현을 포함합니다.
- `main.py`: 모델을 훈련시키고 훈련 손실을 플로팅합니다.
- `generate.py`: 훈련된 모델을 사용하여 시드 문자와 온도 설정에 기반하여 텍스트를 생성합니다.

## 훈련 및 검증 손실
다음 그래프는 Vanilla RNN 및 LSTM 모델의 훈련 손실을 나타냅니다:

![image](https://github.com/undraa0309/Language-model/assets/133347765/20c75bfd-8ff0-490e-bf36-46d19f20cf9e)



## 텍스트 생성
다음은 모델에 의해 생성된 텍스트 샘플입니다:

### Vanilla RNN
- 시드: 'H', 온도: 1.0

### LSTM
- 시드: 'T', 온도: 1.5

## Softmax 온도
온도 매개변수 `T`를 사용한 소프트맥스 함수는 다음과 같습니다:

\[ y_i = \frac{\exp(z_i / T)}{\sum{\exp(z_i / T)}} \]

다양한 온도 설정은 생성된 텍스트의 무작위성에 영향을 미칩니다. 높은 온도는 더 무작위적인 텍스트를 생성하고, 낮은 온도는 더 예측 가능한 텍스트를 생성합니다.

